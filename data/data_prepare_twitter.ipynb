{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze and prepare the twitter dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Source Code Form is subject to the terms of the MPL\n",
    "# License. If a copy of the same was not distributed with this\n",
    "# file, You can obtain one at\n",
    "# https://github.com/akhilpandey95/altpred/blob/master/LICENSE.\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the directory\n",
    "path = '/media/hector/DATA/datalab-data/combined_file/keys/*/*.txt'\n",
    "final = '/media/hector/DATA/datalab-data/twitter_j2018_full.csv'\n",
    "\n",
    "# use glob to read all the files from the path\n",
    "files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Print out the prospective column names for the `posts` category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dict 'posts' dict_keys(['twitter', 'facebook'])\n",
      "Keys in the dict 'posts{twitter}' dict_keys(['license', 'author', 'url', 'citation_ids', 'tweet_id', 'posted_on'])\n",
      "Keys in the dict 'posts{facebook}' dict_keys(['license', 'title', 'url', 'author', 'summary', 'citation_ids', 'posted_on'])\n",
      "Keys in the dict 'posts{blogs}' dict_keys(['title', 'url', 'license', 'citation_ids', 'posted_on', 'author'])\n",
      "Keys in the dict 'posts{policy}' dict_keys(['title', 'url', 'license', 'citation_ids', 'posted_on', 'source', 'page_url', 'collections', 'author'])\n",
      "Keys in the dict 'posts{patent}' dict_keys(['title', 'url', 'license', 'citation_ids', 'posted_on', 'summary', 'ucid', 'jurisdiction', 'filing_status'])\n",
      "Keys in the dict 'posts{googleplus}' dict_keys(['license', 'title', 'url', 'author', 'summary', 'citation_ids', 'posted_on'])\n",
      "Keys in the dict 'posts{wikipedia}' dict_keys(['title', 'url', 'license', 'citation_ids', 'posted_on', 'summary', 'page_url', 'wiki_lang', 'author'])\n"
     ]
    }
   ],
   "source": [
    "## read a sample file\n",
    "f = open('/media/hector/DATA/datalab-data/combined_file/keys/100/10029320.txt').readlines()\n",
    "\n",
    "## check the keys in the dict\n",
    "d_1 = json.loads(f[29])\n",
    "d_2 = json.loads(f[37])\n",
    "d_3 = json.loads(f[39])\n",
    "d_4 = json.loads(f[49])\n",
    "d_5 = json.loads(f[41])\n",
    "d_6 = json.loads(f[20])\n",
    "\n",
    "\n",
    "## print the keys for 'posts'\n",
    "print(\"Keys in the dict 'posts'\", d_1['posts'].keys())\n",
    "\n",
    "## print the keys for twitter\n",
    "print(\"Keys in the dict 'posts{twitter}'\", d_1['posts']['twitter'][0].keys())\n",
    "\n",
    "## print the keys for facebook\n",
    "print(\"Keys in the dict 'posts{facebook}'\", d_1['posts']['facebook'][0].keys())\n",
    "\n",
    "## print the keys for blogs\n",
    "print(\"Keys in the dict 'posts{blogs}'\", d_2['posts']['blogs'][0].keys())\n",
    "\n",
    "## print the keys for policy\n",
    "print(\"Keys in the dict 'posts{policy}'\", d_3['posts']['policy'][0].keys())\n",
    "\n",
    "## print the keys for patent\n",
    "print(\"Keys in the dict 'posts{patent}'\", d_4['posts']['patent'][0].keys())\n",
    "\n",
    "## print the keys for googleplus\n",
    "print(\"Keys in the dict 'posts{googleplus}'\", d_5['posts']['googleplus'][0].keys())\n",
    "\n",
    "## print the keys for wikipedia\n",
    "print(\"Keys in the dict 'posts{wikipedia}'\", d_6['posts']['wikipedia'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Script for writing the data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1960/1960 [02:02<00:00, 16.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# write the file to a csv\n",
    "'''\n",
    "{'rt': ['FrontImmunol'],\n",
    " 'license': 'gnip',\n",
    " 'author': {'name': 'Monica',\n",
    "  'url': 'http://linkedin.com/in/monicascdreis',\n",
    "  'image': 'https://pbs.twimg.com/profile_images/946314369578696704/2TXhbQTE_normal.jpg',\n",
    "  'tweeter_id': '1887350696',\n",
    "  'followers': 607,\n",
    "  'id_on_source': 'Sophitia_Rin',\n",
    "  'geo': {'ln': -71.05977, 'country': 'US', 'lt': 42.35843},\n",
    "  'description': 'Stem cell and EV/Exosome scientist! An european exploring the world! @harvardmed #Immunology #scifigeek #musiclover #worldcitizen'},\n",
    " 'url': 'http://twitter.com/Sophitia_Rin/statuses/905451053340135424',\n",
    " 'citation_ids': [24173673],\n",
    " 'tweet_id': '905451053340135424',\n",
    " 'posted_on': '2017-09-06T15:22:08+00:00'}\n",
    "'''\n",
    "with open(final, 'w') as final_file:\n",
    "    datawriter = csv.writer(final_file, delimiter=',',quotechar='|', \n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    datawriter.writerow(['altmetric_id', 'tweet_id', 'tweet_url',\n",
    "                         'tweet_post_date', 'twitter_author_url', \n",
    "                         'twitter_author_description', 'twitter_author_id',\n",
    "                         'twitter_author_handle', 'twitter_author_followers',\n",
    "                         'twitter_author_name', 'paper_title', 'paper_abstract', \n",
    "                         'paper_doi', 'paper_pubdate', 'paper_subjects', \n",
    "                         'paper_publisher_subjects', 'paper_scopus_subjects'])\n",
    "    \n",
    "    # the exception handling block\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            with open(file) as f:\n",
    "                for text_data in f.readlines():\n",
    "                    data = json.loads(text_data)\n",
    "                    if 'altmetric_id' in data:\n",
    "                        altmetric_id = data['altmetric_id']\n",
    "                        if 'posts' in data and 'twitter' in data['posts']:\n",
    "                            for post in data['posts']['twitter']:\n",
    "                                tweet_id = []\n",
    "                                tweet_url = [] \n",
    "                                tweet_post_date = [] \n",
    "                                twitter_author_url = []\n",
    "                                twitter_author_description = []\n",
    "                                twitter_author_id = []\n",
    "                                twitter_author_handle = []\n",
    "                                twitter_author_followers = []\n",
    "                                twitter_author_name = []\n",
    "                                if 'tweet_id' in post:\n",
    "                                    tweet_id.append(str(post['tweet_id']))\n",
    "                                else:\n",
    "                                    tweet_id = np.nan\n",
    "                                if 'url' in post:\n",
    "                                    tweet_url.append(str(post['url']))\n",
    "                                else:\n",
    "                                    tweet_url = np.nan\n",
    "                                if 'posted_on' in post:\n",
    "                                    tweet_post_date.append(str(post['posted_on']))\n",
    "                                else:\n",
    "                                    tweet_post_date = np.nan\n",
    "                                if 'author' in post and 'url' in post['author']:\n",
    "                                    twitter_author_url.append(str(post['author']['url']))\n",
    "                                else:\n",
    "                                    twitter_author_url = np.nan\n",
    "                                if 'author' in post and 'description' in post['author']:\n",
    "                                    twitter_author_description.append(str(post['author']['description']))\n",
    "                                else:\n",
    "                                    twitter_author_description = np.nan\n",
    "                                if 'author' in post and 'tweeter_id' in post['author']:\n",
    "                                    twitter_author_id.append(post['author']['tweeter_id'])\n",
    "                                else:\n",
    "                                    twitter_author_id = np.nan\n",
    "                                #\n",
    "                                if 'author' in post and 'id_on_source' in post['author']:\n",
    "                                    twitter_author_handle.append(post['author']['id_on_source'])\n",
    "                                else:\n",
    "                                    twitter_author_handle = np.nan\n",
    "                                if 'author' in post and 'followers' in post['author']:\n",
    "                                    twitter_author_followers.append(post['author']['followers'])\n",
    "                                else:\n",
    "                                    twitter_author_followers = np.nan\n",
    "                                if 'author' in post and 'name' in post['author']:\n",
    "                                    twitter_author_name.append(post['author']['name'])\n",
    "                                else:\n",
    "                                    twitter_author_name = np.nan\n",
    "                            if 'title' in data['citation']:\n",
    "                                paper_title = str(data['citation']['title'])\n",
    "                            else:\n",
    "                                paper_title = np.nan\n",
    "                            if 'abstract' in data['citation']:\n",
    "                                paper_abstract = str(data['citation']['abstract'])\n",
    "                            else:\n",
    "                                paper_abstract = np.nan\n",
    "                            if 'doi' in data['citation']:\n",
    "                                paper_doi = str(data['citation']['doi'])\n",
    "                            else:\n",
    "                                paper_doi = np.nan\n",
    "                            if 'pubdate' in data['citation']:\n",
    "                                paper_pubdate = str(data['citation']['pubdate'])\n",
    "                            else:\n",
    "                                paper_pubdate = np.nan\n",
    "                            if 'subjects' in data['citation']:\n",
    "                                paper_subjects = str(data['citation']['subjects'])\n",
    "                            else:\n",
    "                                paper_subjects = np.nan\n",
    "                            if 'publisher_subjects' in data['citation']:\n",
    "                                paper_publisher_subjects = str(data['citation']['publisher_subjects'])\n",
    "                            else:\n",
    "                                paper_publisher_subjects = np.nan\n",
    "                            if 'scopus_subjects' in data['citation']:\n",
    "                                paper_scopus_subjects = str(data['citation']['scopus_subjects'])\n",
    "                            else:\n",
    "                                paper_scopus_subjects = np.nan\n",
    "                            datawriter.writerow([altmetric_id, tweet_id, tweet_url,\n",
    "                         tweet_post_date, twitter_author_url, \n",
    "                         twitter_author_description, twitter_author_id,\n",
    "                         twitter_author_handle, twitter_author_followers,\n",
    "                         twitter_author_name, paper_title, paper_abstract, \n",
    "                         paper_doi, paper_pubdate, paper_subjects, \n",
    "                         paper_publisher_subjects, paper_scopus_subjects])\n",
    "        except IOError as exc:\n",
    "            if exc.errno != errno.EISDIR:\n",
    "                raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
