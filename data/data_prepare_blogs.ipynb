{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze and prepare the blogs dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Source Code Form is subject to the terms of the MPL\n",
    "# License. If a copy of the same was not distributed with this\n",
    "# file, You can obtain one at\n",
    "# https://github.com/akhilpandey95/altpred/blob/master/LICENSE.\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the directory\n",
    "path = '/media/hector/DATA/datalab-data/combined_file/keys/*/*.txt'\n",
    "final = '/media/hector/DATA/datalab-data/blogs_j2018_full.csv'\n",
    "\n",
    "# use glob to read all the files from the path\n",
    "files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Print out the prospective column names for the `posts` category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the dict 'posts{blogs}' dict_keys(['license', 'title', 'url', 'author', 'summary', 'citation_ids', 'posted_on'])\n"
     ]
    }
   ],
   "source": [
    "## read a sample file\n",
    "f = open('/media/hector/DATA/datalab-data/combined_file/keys/100/10000556.txt').readlines()\n",
    "\n",
    "## check the keys in the dict\n",
    "d_1 = json.loads(f[19])\n",
    "\n",
    "## print the keys for blogs\n",
    "print(\"Keys in the dict 'posts{blogs}'\", d_1['posts']['blogs'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Script for writing the data into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380518/380518 [2:57:47<00:00, 35.67it/s]  \n"
     ]
    }
   ],
   "source": [
    "# write the file to a csv\n",
    "'''\n",
    "{'title': 'Vitamin D deficiency and Chronic Fatigue Syndrome',\n",
    " 'url': 'http://questioning-answers.blogspot.com/2012/10/vitamin-d-deficiency-and-chronic-fatigue-syndrome.html',\n",
    " 'license': 'public',\n",
    " 'citation_ids': [1200711,\n",
    "  1485287,\n",
    "  253880,\n",
    "  1485289,\n",
    "  1485290,blogs_j2018_full\n",
    "  1485291,\n",
    "  1485292],\n",
    " 'posted_on': '2012-10-08T14:02:00+00:00',\n",
    " 'summary': 'Vitamin D3 @ Wikipedia\\xa0I have no hesitation in admitting to being more than a little bit \"obsessive\" about my data capture and the volumes of lovely, yummy science-y research alerts it produces. Indeed quite a bit of said research ends up as fodder for thi',\n",
    " 'author': {'name': 'Questioning Answers',\n",
    "  'url': 'http://questioning-answers.blogspot.com/',\n",
    "  'description': 'Views on autism research and other musings.'}}\n",
    "'''\n",
    "with open(final, 'w') as final_file:\n",
    "    datawriter = csv.writer(final_file, delimiter=',',quotechar='|', \n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    datawriter.writerow(['altmetric_id', 'blog_title', 'blog_url',\n",
    "                         'blog_summary', 'blog_pub_on', 'blog_author_name'\n",
    "                         'blog_author_url', 'paper_title', 'paper_abstract', \n",
    "                         'paper_doi', 'paper_pubdate', 'paper_subjects', \n",
    "                         'paper_publisher_subjects', 'paper_scopus_subjects'])\n",
    "    \n",
    "    # the exception handling block\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            with open(file) as f:\n",
    "                for text_data in f.readlines():\n",
    "                    data = json.loads(text_data)\n",
    "                    if 'altmetric_id' in data:\n",
    "                        altmetric_id = data['altmetric_id']\n",
    "                        if 'posts' in data and 'blogs' in data['posts']:\n",
    "                            for post in data['posts']['blogs']:\n",
    "                                blog_title = []\n",
    "                                blog_url = [] \n",
    "                                blog_summary = [] \n",
    "                                blog_pub_on = []\n",
    "                                blog_author_name = []\n",
    "                                blog_author_url = []\n",
    "                                if 'title' in post:\n",
    "                                    blog_title.append(str(post['title']))\n",
    "                                else:\n",
    "                                    blog_title = np.nan\n",
    "                                if 'url' in post:\n",
    "                                    blog_url.append(str(post['url']))\n",
    "                                else:\n",
    "                                    blog_url = np.nan\n",
    "                                if 'summary' in post:\n",
    "                                    blog_summary.append(str(post['summary']))\n",
    "                                else:\n",
    "                                    blog_summary = np.nan\n",
    "                                if 'posted_on' in post:\n",
    "                                    blog_pub_on.append(str(post['posted_on']))\n",
    "                                else:\n",
    "                                    blog_pub_on = np.nan\n",
    "                                if 'author' in post and 'name' in post['author']:\n",
    "                                    blog_author_name.append(post['author']['name'])\n",
    "                                else:\n",
    "                                    blog_author_name = np.nan\n",
    "                                if 'author' in post and 'url' in post['author']:\n",
    "                                    blog_author_url.append(str(post['author']['url']))\n",
    "                                else:\n",
    "                                    blog_author_url = np.nan\n",
    "                            if 'title' in data['citation']:\n",
    "                                paper_title = str(data['citation']['title'])\n",
    "                            else:\n",
    "                                paper_title = np.nan\n",
    "                            if 'abstract' in data['citation']:\n",
    "                                paper_abstract = str(data['citation']['abstract'])\n",
    "                            else:\n",
    "                                paper_abstract = np.nan\n",
    "                            if 'doi' in data['citation']:\n",
    "                                paper_doi = str(data['citation']['doi'])\n",
    "                            else:\n",
    "                                paper_doi = np.nan\n",
    "                            if 'pubdate' in data['citation']:\n",
    "                                paper_pubdate = str(data['citation']['pubdate'])\n",
    "                            else:\n",
    "                                paper_pubdate = np.nan\n",
    "                            if 'subjects' in data['citation']:\n",
    "                                paper_subjects = str(data['citation']['subjects'])\n",
    "                            else:\n",
    "                                paper_subjects = np.nan\n",
    "                            if 'publisher_subjects' in data['citation']:\n",
    "                                paper_publisher_subjects = str(data['citation']['publisher_subjects'])\n",
    "                            else:\n",
    "                                paper_publisher_subjects = np.nan\n",
    "                            if 'scopus_subjects' in data['citation']:\n",
    "                                paper_scopus_subjects = str(data['citation']['scopus_subjects'])\n",
    "                            else:\n",
    "                                paper_scopus_subjects = np.nan\n",
    "                            datawriter.writerow([altmetric_id, blog_title, blog_url, blog_summary, blog_pub_on, blog_author_name, blog_author_url, paper_title, paper_abstract, paper_doi, paper_pubdate,paper_subjects, paper_publisher_subjects, paper_scopus_subjects])\n",
    "        except IOError as exc:\n",
    "            if exc.errno != errno.EISDIR:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
